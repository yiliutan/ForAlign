{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "873aefe9-bb22-4efc-b710-86b4f4e668b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ForAlign: An optimization algorithm for forest point cloud registration (Notebook Version)\n",
    "# Coarse Alignment algorithm\n",
    "\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "import pulp\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from itertools import combinations\n",
    "import rasterio\n",
    "import hdbscan\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d0af214-0eca-4ac2-b678-d48cf3d2c2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# I/O Handling Module\n",
    "# ==========================\n",
    "def load_pcd(file_path):\n",
    "    \"\"\" Load a point cloud from a given file path. \"\"\"\n",
    "    return o3d.io.read_point_cloud(file_path)\n",
    "\n",
    "def save_pcd(file_path, pcd):\n",
    "    \"\"\" Save a point cloud to a specified file path. \"\"\"\n",
    "    o3d.io.write_point_cloud(file_path, pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a68e3f1a-72da-4b7c-bcbc-cc7331cf33ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# DTM Processing Module\n",
    "# ==========================\n",
    "def generate_dtm(ground_pcd, resolution=0.5, sigma=1):\n",
    "    \"\"\"\n",
    "    Generate a smoothed Digital Terrain Model (DTM) from ground points.\n",
    "\n",
    "    Parameters:\n",
    "    - ground_pcd: Open3D PointCloud object representing ground points.\n",
    "    - resolution: Grid resolution (default: 0.5 meters).\n",
    "    - sigma: Gaussian filter sigma for smoothing (default: 1).\n",
    "\n",
    "    Returns:\n",
    "    - x_mesh, y_mesh: Mesh grid for terrain coordinates.\n",
    "    - z_values_smoothed: Smoothed elevation values for the terrain.\n",
    "    \"\"\"\n",
    "    ground_points = np.asarray(ground_pcd.points)\n",
    "    \n",
    "    # Compute min/max bounds\n",
    "    x_min, y_min = np.min(ground_points, axis=0)[:2]\n",
    "    x_max, y_max = np.max(ground_points, axis=0)[:2]\n",
    "\n",
    "    # Generate grid\n",
    "    x_grid = np.arange(x_min, x_max, resolution, dtype=np.float64)\n",
    "    y_grid = np.arange(y_min, y_max, resolution, dtype=np.float64)\n",
    "    x_mesh, y_mesh = np.meshgrid(x_grid, y_grid)\n",
    "\n",
    "    # Interpolate and smooth terrain data\n",
    "    z_values = griddata(ground_points[:, :2], ground_points[:, 2], (x_mesh, y_mesh), method='linear')\n",
    "    z_values_smoothed = gaussian_filter(z_values, sigma=sigma)\n",
    "\n",
    "    return x_mesh, y_mesh, z_values_smoothed\n",
    "\n",
    "# ==========================\n",
    "# Normalized Height Calculation\n",
    "# ==========================\n",
    "def extract_normalized_height(offground_pcd, dtm_data):\n",
    "    \"\"\"\n",
    "    Compute the normalized height of off-ground points by subtracting DTM elevation.\n",
    "\n",
    "    Parameters:\n",
    "    - offground_pcd: Open3D PointCloud object containing off-ground points.\n",
    "    - dtm_data: Dictionary containing x_mesh, y_mesh, and smoothed elevation data.\n",
    "\n",
    "    Returns:\n",
    "    - normalized_elevation: Array of normalized height values.\n",
    "    \"\"\"\n",
    "    offground_points = np.asarray(offground_pcd.points)\n",
    "    x_mesh, y_mesh, z_values_smoothed = dtm_data[\"x_mesh\"], dtm_data[\"y_mesh\"], dtm_data[\"z_values_smoothed\"]\n",
    "    \n",
    "    normalized_elevation = np.zeros(len(offground_points))\n",
    "\n",
    "    for i, point in enumerate(offground_points):\n",
    "        # Find closest grid index in DTM\n",
    "        idx_x = (np.abs(x_mesh[0] - point[0])).argmin()\n",
    "        idx_y = (np.abs(y_mesh[:, 0] - point[1])).argmin()\n",
    "        \n",
    "        # Compute relative height above ground\n",
    "        normalized_elevation[i] = point[2] - z_values_smoothed[idx_y, idx_x]\n",
    "\n",
    "    return normalized_elevation\n",
    "\n",
    "# ==========================\n",
    "# Tree Trunk Extraction Module\n",
    "# ==========================\n",
    "def filter_points_by_height(offground_pcd, normalized_elevation, elevation_min=4.5, elevation_max=5):\n",
    "    \"\"\"\n",
    "    Extract tree trunk points by filtering off-ground points based on their normalized height.\n",
    "\n",
    "    Parameters:\n",
    "    - offground_pcd: Open3D PointCloud object containing off-ground points.\n",
    "    - normalized_elevation: Array of height values for each point.\n",
    "    - elevation_min: Minimum height threshold for filtering.\n",
    "    - elevation_max: Maximum height threshold for filtering.\n",
    "\n",
    "    Returns:\n",
    "    - filtered_pcd: Open3D PointCloud object containing extracted tree trunk points.\n",
    "    \"\"\"\n",
    "    offground_points = np.asarray(offground_pcd.points)\n",
    "    indices_in_range = (normalized_elevation >= elevation_min) & (normalized_elevation <= elevation_max)\n",
    "    filtered_points = offground_points[indices_in_range]\n",
    "    \n",
    "    filtered_pcd = o3d.geometry.PointCloud()\n",
    "    filtered_pcd.points = o3d.utility.Vector3dVector(filtered_points)\n",
    "    return filtered_pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bbd95c8-841d-4bc8-be71-a9df66039c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Tree Trunk Clustering with HDBSCAN\n",
    "# ==========================\n",
    "def apply_hdbscan(points, min_cluster_size=3, min_samples=100, cluster_selection_epsilon=0.5):\n",
    "    \"\"\"\n",
    "    Perform HDBSCAN clustering on a point cloud.\n",
    "\n",
    "    Parameters:\n",
    "    - points: np.array, the set of points to cluster.\n",
    "    - min_cluster_size: Minimum cluster size (default=3).\n",
    "    - min_samples: Minimum samples per cluster (default=100).\n",
    "    - cluster_selection_epsilon: Minimum spacing between clusters (default=0.5).\n",
    "\n",
    "    Returns:\n",
    "    - labels: np.array, cluster labels for each point.\n",
    "    \"\"\"\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size, \n",
    "        min_samples=min_samples, \n",
    "        cluster_selection_epsilon=cluster_selection_epsilon\n",
    "    )\n",
    "    labels = clusterer.fit_predict(points)\n",
    "    return labels\n",
    "\n",
    "# ==========================\n",
    "# Cluster Analysis and Scoring\n",
    "# ==========================\n",
    "def analyze_clusters(points, labels):\n",
    "    \"\"\"\n",
    "    Compute properties of each cluster, including point count, linearity, and vertical alignment.\n",
    "\n",
    "    Parameters:\n",
    "    - points: np.array, original point cloud data.\n",
    "    - labels: np.array, cluster labels assigned by HDBSCAN.\n",
    "\n",
    "    Returns:\n",
    "    - clusters_data: dict, mapping cluster IDs to extracted properties.\n",
    "    \"\"\"\n",
    "    clusters_data = {}\n",
    "    total_points = len(points[labels != -1])  # Ignore noise\n",
    "    z_axis = np.array([0, 0, 1])  # Reference Z-axis\n",
    "\n",
    "    for label in np.unique(labels):\n",
    "        if label == -1:\n",
    "            continue  # Skip noise\n",
    "        cluster_points = points[labels == label]\n",
    "        count = len(cluster_points)\n",
    "        normalized_count = count / total_points\n",
    "\n",
    "        # Compute PCA to measure linearity and vertical alignment\n",
    "        if len(cluster_points) > 1:\n",
    "            pca = PCA(n_components=min(3, len(cluster_points)))\n",
    "            pca.fit(cluster_points)\n",
    "            linearity = pca.explained_variance_ratio_[0]\n",
    "            pc1 = pca.components_[0]\n",
    "            vertical_alignment = abs(np.dot(pc1, z_axis))\n",
    "        else:\n",
    "            linearity = 1\n",
    "            vertical_alignment = 1 if np.dot(cluster_points[0], z_axis) > 0 else 0\n",
    "\n",
    "        clusters_data[label] = {\n",
    "            'count': count,\n",
    "            'normalized_count': normalized_count,\n",
    "            'linearity': linearity,\n",
    "            'vertical_alignment': vertical_alignment\n",
    "        }\n",
    "    \n",
    "    return clusters_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc618ff3-5a8a-437e-86e9-dbb23271267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_centroid(cluster_points):\n",
    "    \"\"\" Calculate the centroid of a cluster. \"\"\"\n",
    "    if cluster_points.ndim == 1:\n",
    "        return cluster_points\n",
    "    return np.mean(cluster_points, axis=0)\n",
    "\n",
    "def project_to_xy(centroid):\n",
    "    \"\"\" Project a centroid to the XY plane. \"\"\"\n",
    "    return centroid[:2]\n",
    "\n",
    "def calculate_angle(v1, v2):\n",
    "    \"\"\" Calculate the angle in radians between vectors 'v1' and 'v2'. \"\"\"\n",
    "    cos_theta = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    angle = np.arccos(np.clip(cos_theta, -1, 1))  # Clip for numerical stability\n",
    "    return angle\n",
    "\n",
    "# ==========================\n",
    "# Graph Construction for Tree Matching\n",
    "# ==========================\n",
    "def construct_graph_features(clusters, k=5):\n",
    "    \"\"\"\n",
    "    Construct a kNN-based graph from tree trunk cluster centroids.\n",
    "\n",
    "    Parameters:\n",
    "    - clusters: list of np.array, tree trunk clusters.\n",
    "    - k: int, number of nearest neighbors (default=5).\n",
    "\n",
    "    Returns:\n",
    "    - graph_features: dict, containing neighbor relationships and angles.\n",
    "    - node_to_node_distances: dict, storing pairwise distances.\n",
    "    \"\"\"\n",
    "    graph_features = {}\n",
    "    node_to_node_distances = {}\n",
    "\n",
    "    # Compute cluster centroids and project to XY plane\n",
    "    centroids = np.array([np.mean(cluster, axis=0) for cluster in clusters])\n",
    "    nodes = centroids[:, :2]  # Project to XY\n",
    "\n",
    "    # Apply kNN\n",
    "    knn = NearestNeighbors(n_neighbors=k+1)\n",
    "    knn.fit(nodes)\n",
    "    distances, indices = knn.kneighbors(nodes)\n",
    "\n",
    "    for i, (centroid, neighbors_idx) in enumerate(zip(centroids, indices)):\n",
    "        neighbor_indices = neighbors_idx[1:]\n",
    "        neighbor_nodes = nodes[neighbor_indices]\n",
    "\n",
    "        # Store distances\n",
    "        for j, neighbor_idx in enumerate(neighbor_indices):\n",
    "            node_to_node_distances[(i, neighbor_idx)] = distances[i][j+1]\n",
    "\n",
    "        # Compute angles\n",
    "        neighbor_combinations = list(combinations(neighbor_indices, 2))\n",
    "        angles = [\n",
    "            np.arctan2(nodes[n2][1] - centroid[1], nodes[n2][0] - centroid[0]) -\n",
    "            np.arctan2(nodes[n1][1] - centroid[1], nodes[n1][0] - centroid[0])\n",
    "            for n1, n2 in neighbor_combinations\n",
    "        ]\n",
    "        angles = np.mod(angles, 2 * np.pi)  # Ensure within [0, 2Ï€)\n",
    "\n",
    "        graph_features[i] = {\n",
    "            'combinations': neighbor_combinations,\n",
    "            'angles': angles\n",
    "        }\n",
    "\n",
    "    return graph_features, node_to_node_distances\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f32a9ed-18e1-419d-acff-2edd8769aec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_within_threshold(value1, value2, threshold):\n",
    "    \"\"\" Check if two values are within a given threshold. \"\"\"\n",
    "    return abs(value1 - value2) <= threshold\n",
    "\n",
    "def angle_difference(angle1, angle2):\n",
    "    \"\"\" Compute the absolute difference between two angles, considering circularity. \"\"\"\n",
    "    return min(abs(angle1 - angle2), 2 * np.pi - abs(angle1 - angle2))\n",
    "\n",
    "# ==========================\n",
    "# Graph Matching\n",
    "# ==========================\n",
    "def match_sets(dls_features, tls_features, dls_node_to_node_distances, tls_node_to_node_distances, distance_threshold, angle_threshold_degrees):\n",
    "    \"\"\"\n",
    "    Match tree trunk clusters using graph-based similarity.\n",
    "\n",
    "    Parameters:\n",
    "    - dls_features, tls_features: dict, graph features for DLS and TLS datasets.\n",
    "    - dls_node_to_node_distances, tls_node_to_node_distances: dict, distances between nodes.\n",
    "    - distance_threshold: float, distance matching threshold.\n",
    "    - angle_threshold_degrees: float, angle matching threshold in degrees.\n",
    "\n",
    "    Returns:\n",
    "    - matches: list, containing matched graph nodes.\n",
    "    \"\"\"\n",
    "    angle_threshold = np.radians(angle_threshold_degrees)\n",
    "    matches = []\n",
    "\n",
    "    for dls_axis, dls_data in dls_features.items():\n",
    "        for tls_axis, tls_data in tls_features.items():\n",
    "            for dls_combination, tls_combination in zip(dls_data['combinations'], tls_data['combinations']):\n",
    "                dls_distances = [dls_node_to_node_distances[(dls_axis, n)] for n in dls_combination]\n",
    "                tls_distances = [tls_node_to_node_distances[(tls_axis, n)] for n in tls_combination]\n",
    "\n",
    "                # Match based on distance and angle threshold\n",
    "                distance_match = all(abs(d - t) <= distance_threshold for d, t in zip(dls_distances, tls_distances))\n",
    "                angle_match = all(abs(d - t) <= angle_threshold for d, t in zip(dls_data['angles'], tls_data['angles']))\n",
    "\n",
    "                if distance_match and angle_match:\n",
    "                    matches.append((dls_axis, tls_axis, dls_combination, tls_combination))\n",
    "    \n",
    "    return matches\n",
    "    \n",
    "\n",
    "def standardize_match(match):\n",
    "    \"\"\" Standardize match representation for consistency. \"\"\"\n",
    "    branch_pairs = tuple(sorted([(match['dls_combination'][i], match['tls_combination'][i]) for i in range(2)]))\n",
    "    return ((match['dls_axis'], match['tls_axis']),) + branch_pairs\n",
    "\n",
    "def record_matches(matched_sets):\n",
    "    \"\"\" Record all matched sets. \"\"\"\n",
    "    return [standardize_match(match) for match in matched_sets]\n",
    "\n",
    "# ==========================\n",
    "# Pattern Recognition\n",
    "# ==========================\n",
    "def find_complete_patterns(recorded_matches):\n",
    "    \"\"\"\n",
    "    Identify complete graph matching patterns.\n",
    "\n",
    "    Parameters:\n",
    "    - recorded_matches: list, containing matched graph nodes.\n",
    "\n",
    "    Returns:\n",
    "    - complete_patterns: list, containing full matching structures.\n",
    "    \"\"\"\n",
    "    patterns = {}\n",
    "    \n",
    "    for match in recorded_matches:\n",
    "        nodes = frozenset(match)\n",
    "        patterns.setdefault(nodes, []).append(match)\n",
    "    \n",
    "    return [matches[0] for nodes, matches in patterns.items() if len(matches) == 3 and len(set(map(frozenset, matches))) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b03b95cd-1fd7-4f37-8028-68906806d694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Graph Matching Optimization via Linear Programming\n",
    "# ==========================\n",
    "def optimize_graph_matching(complete_patterns):\n",
    "    \"\"\"\n",
    "    Perform graph matching optimization using linear programming (LP).\n",
    "    \n",
    "    The goal is to maximize the number of valid matches while ensuring \n",
    "    one-to-one correspondence between matched nodes.\n",
    "\n",
    "    Parameters:\n",
    "    - complete_patterns: list, containing all valid graph matching patterns.\n",
    "\n",
    "    Returns:\n",
    "    - matched_x_pairs: list of tuples, representing matched node pairs.\n",
    "    - matched_y_patterns: list of tuples, representing optimized matching sets.\n",
    "    \"\"\"\n",
    "    prob = pulp.LpProblem(\"GraphMatching\", pulp.LpMaximize)\n",
    "\n",
    "    # Define decision variables\n",
    "    y_vars = {}\n",
    "    x_vars = {}\n",
    "    unique_mn_pairs = set(pair for pattern in complete_patterns for pair in pattern)\n",
    "\n",
    "    # Binary variables for node matching\n",
    "    for m, n in unique_mn_pairs:\n",
    "        x_vars[(m, n)] = pulp.LpVariable(f\"x_{m}_{n}\", cat='Binary')\n",
    "\n",
    "    # Binary variables for pattern selection\n",
    "    for c, pattern in enumerate(complete_patterns):\n",
    "        for m, n in pattern: \n",
    "            y_vars[(m, n, c)] = pulp.LpVariable(f\"y_{m}_{n}_{c}\", cat='Binary')\n",
    "\n",
    "    # Objective: Maximize the total number of matched patterns\n",
    "    prob += pulp.lpSum(y_vars.values())\n",
    "\n",
    "    # Ensure y_vars cannot be 1 unless x_vars is 1\n",
    "    for (m, n, c), y_var in y_vars.items():\n",
    "        prob += y_var <= x_vars[(m, n)]\n",
    "\n",
    "    # Ensure x_vars is set if any y_var is set for a pattern\n",
    "    for m, n in unique_mn_pairs:\n",
    "        prob += pulp.lpSum(y_vars[(m, n, c)] for c in range(len(complete_patterns)) if (m, n, c) in y_vars) >= x_vars[(m, n)]\n",
    "\n",
    "    # Ensure each node is matched at most once\n",
    "    for m in set(pair[0] for pair in unique_mn_pairs):\n",
    "        prob += pulp.lpSum(x_vars[(m, n)] for n in set(pair[1] for pair in unique_mn_pairs) if (m, n) in x_vars) <= 1\n",
    "\n",
    "    for n in set(pair[1] for pair in unique_mn_pairs):\n",
    "        prob += pulp.lpSum(x_vars[(m, n)] for m in set(pair[0] for pair in unique_mn_pairs) if (m, n) in x_vars) <= 1\n",
    "\n",
    "    # Ensure consistency within each pattern\n",
    "    for c, pattern in enumerate(complete_patterns):\n",
    "        for i, (m1, n1) in enumerate(pattern):\n",
    "            for j, (m2, n2) in enumerate(pattern):\n",
    "                if i < j:\n",
    "                    prob += y_vars[(m1, n1, c)] == y_vars[(m2, n2, c)]\n",
    "\n",
    "    # Solve the optimization problem\n",
    "    prob.solve()\n",
    "\n",
    "    # Extract optimized matches\n",
    "    matched_x_pairs = [k for k, v in x_vars.items() if v.varValue == 1]\n",
    "    matched_y_patterns = [(m, n, c) for (m, n, c), v in y_vars.items() if v.varValue == 1]\n",
    "\n",
    "    return matched_x_pairs, matched_y_patterns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Elevation Retrieval from DTM\n",
    "# ==========================\n",
    "def get_elevation_at_points(dtm_file, points_2d):\n",
    "    \"\"\"\n",
    "    Retrieve elevation values from a Digital Terrain Model (DTM) for given 2D points.\n",
    "\n",
    "    Parameters:\n",
    "    - dtm_file: str, file path to the DTM raster.\n",
    "    - points_2d: np.array (Nx2), 2D coordinates (x, y) for elevation lookup.\n",
    "\n",
    "    Returns:\n",
    "    - elevations: np.array (N,), corresponding elevation values.\n",
    "    \"\"\"\n",
    "    with rasterio.open(dtm_file) as dtm:\n",
    "        row_indices, col_indices = dtm.index(points_2d[:, 0], points_2d[:, 1])\n",
    "        elevation_data = dtm.read(1)  # Read first band\n",
    "        elevations = elevation_data[row_indices, col_indices]\n",
    "    return elevations\n",
    "\n",
    "\n",
    "    \n",
    "def get_z_elevation_from_dtm(nodes, dtm_data, pattern_indices):\n",
    "    \"\"\"\n",
    "    Get Z elevation values from a smoothed Digital Terrain Model (DTM) using nearest neighbor interpolation.\n",
    "\n",
    "    Parameters:\n",
    "    - nodes: np.array, XY positions of tree clusters.\n",
    "    - dtm_data: dict, contains precomputed DTM grid ('x_mesh', 'y_mesh', 'z_values_smoothed').\n",
    "    - pattern_indices: list of int, indices of nodes requiring Z values.\n",
    "\n",
    "    Returns:\n",
    "    - points_3d: np.array (Nx3), 3D points with updated Z elevations.\n",
    "    \"\"\"\n",
    "    points_2d = np.array([nodes[i] for i in pattern_indices])\n",
    "    x_mesh, y_mesh, z_values_smoothed = dtm_data['x_mesh'], dtm_data['y_mesh'], dtm_data['z_values_smoothed']\n",
    "\n",
    "    points_3d = []\n",
    "    for point in points_2d:\n",
    "        idx_x = np.abs(x_mesh[0] - point[0]).argmin()\n",
    "        idx_y = np.abs(y_mesh[:, 0] - point[1]).argmin()\n",
    "        z_elevation = z_values_smoothed[idx_y, idx_x]\n",
    "        points_3d.append([point[0], point[1], z_elevation])\n",
    "\n",
    "    return np.array(points_3d)\n",
    "\n",
    "    \n",
    "\n",
    "# ==========================\n",
    "# Rigid Body Transformation Estimation\n",
    "# ==========================\n",
    "def find_rigid_body_transformation(src_points, dst_points):\n",
    "    \"\"\"\n",
    "    Compute a rigid-body transformation (rotation + translation) using Singular Value Decomposition (SVD).\n",
    "\n",
    "    Parameters:\n",
    "    - src_points: np.array (Nx3), source 3D point set.\n",
    "    - dst_points: np.array (Nx3), target 3D point set.\n",
    "\n",
    "    Returns:\n",
    "    - R: np.array (3x3), rotation matrix.\n",
    "    - t: np.array (3,), translation vector.\n",
    "    \"\"\"\n",
    "    src_centroid = np.mean(src_points, axis=0)\n",
    "    dst_centroid = np.mean(dst_points, axis=0)\n",
    "    \n",
    "    src_centered = src_points - src_centroid\n",
    "    dst_centered = dst_points - dst_centroid\n",
    "    \n",
    "    # Compute covariance matrix\n",
    "    H = src_centered.T @ dst_centered\n",
    "    \n",
    "    # Singular Value Decomposition (SVD)\n",
    "    U, S, Vt = np.linalg.svd(H)\n",
    "    R = Vt.T @ U.T\n",
    "\n",
    "    # Ensure proper rotation matrix (no reflection)\n",
    "    if np.linalg.det(R) < 0:\n",
    "        Vt[-1, :] *= -1\n",
    "        R = Vt.T @ U.T\n",
    "    \n",
    "    # Compute translation vector\n",
    "    t = dst_centroid - R @ src_centroid\n",
    "\n",
    "    return R, t\n",
    "\n",
    "    \n",
    "\n",
    "def compute_transformation_matrix(dls_3d_points, tls_3d_points):\n",
    "    \"\"\"\n",
    "    Compute the final 4x4 transformation matrix for aligning two point clouds.\n",
    "\n",
    "    Parameters:\n",
    "    - dls_3d_points: np.array (Nx3), source point cloud (DLS).\n",
    "    - tls_3d_points: np.array (Nx3), target point cloud (TLS).\n",
    "\n",
    "    Returns:\n",
    "    - transformation_matrix: np.array (4x4), transformation matrix (R | t).\n",
    "    \"\"\"\n",
    "    R, t = find_rigid_body_transformation(dls_3d_points, tls_3d_points)\n",
    "\n",
    "    # Construct 4x4 homogeneous transformation matrix\n",
    "    transformation_matrix = np.eye(4)\n",
    "    transformation_matrix[:3, :3] = R\n",
    "    transformation_matrix[:3, 3] = t\n",
    "\n",
    "    return transformation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da11617-2409-43f4-8dc1-828459b5ee70",
   "metadata": {},
   "source": [
    "# Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d843c0e7-a29f-423a-9ad5-0c17fdaf6b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\" \n",
    "    Main function to execute the graph matching and transformation process.\n",
    "    The workflow consists of:\n",
    "    \n",
    "    1. Loading TLS and DLS point clouds (ground and off-ground).\n",
    "    2. Generating Digital Terrain Models (DTM) from ground points.\n",
    "    3. Computing normalized heights for off-ground points.\n",
    "    4. Extracting tree trunks by filtering points within a height range.\n",
    "    5. Performing clustering on tree trunks using HDBSCAN.\n",
    "    6. Analyzing and scoring clusters based on geometric properties.\n",
    "    7. Selecting top clusters and constructing a kNN-based graph.\n",
    "    8. Matching trees using structural graph-based constraints.\n",
    "    9. Optimizing matching results with linear programming.\n",
    "    10. Computing the transformation matrix from matched trees.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define paths for ground and off-ground point clouds\n",
    "    tls_g = \"your_path_to_TLS_ground_point_cloud.pcd\"  # TLS ground points\n",
    "    tls_og = \"your_path_to_TLS_offground_point_cloud.pcd\"  # TLS off-ground points\n",
    "    dls_g = \"your_path_to_DLS_ground_point_cloud.pcd\"  # DLS ground points\n",
    "    dls_og = \"your_path_to_DLS_offground_point_cloud.pcd\"  # DLS off-ground points\n",
    "\n",
    "    \n",
    "    # Load ground and off-ground point clouds for TLS and DLS\n",
    "    tls_ground_pcd = load_pcd(tls_g)\n",
    "    tls_offground_pcd = load_pcd(tls_og)\n",
    "    dls_ground_pcd = load_pcd(dls_g)\n",
    "    dls_offground_pcd = load_pcd(dls_og)\n",
    "\n",
    "    # Generate Digital Terrain Models (DTM) for TLS and DLS\n",
    "    tls_dtm_data = generate_dtm(tls_ground_pcd)\n",
    "    dls_dtm_data = generate_dtm(dls_ground_pcd)\n",
    "\n",
    "    # Compute normalized heights by subtracting DTM elevation from off-ground points\n",
    "    tls_normalized_height = extract_normalized_height(tls_offground_pcd, tls_dtm_data)\n",
    "    dls_normalized_height = extract_normalized_height(dls_offground_pcd, dls_dtm_data)\n",
    "\n",
    "    # Ensure the extracted heights match the input point count\n",
    "    assert len(tls_normalized_height) == len(np.asarray(tls_offground_pcd.points)), \"TLS: Mismatch in points and normalized elevation\"\n",
    "    assert len(dls_normalized_height) == len(np.asarray(dls_offground_pcd.points)), \"DLS: Mismatch in points and normalized elevation\"\n",
    "\n",
    "    # Filter tree trunks by selecting points within a specific height range\n",
    "    tls_stems_pcd = filter_points_by_height(tls_offground_pcd, tls_normalized_height, elevation_min=4.5, elevation_max=5)\n",
    "    dls_stems_pcd = filter_points_by_height(dls_offground_pcd, dls_normalized_height, elevation_min=7.5, elevation_max=8.5)\n",
    "\n",
    "    # Convert tree trunk point clouds to NumPy arrays\n",
    "    tls_stems_points = np.asarray(tls_stems_pcd.points)\n",
    "    dls_stems_points = np.asarray(dls_stems_pcd.points)\n",
    "\n",
    "    # Perform tree trunk clustering using HDBSCAN\n",
    "    tls_clusters = apply_hdbscan(tls_stems_points)\n",
    "    dls_clusters = apply_hdbscan(dls_stems_points)\n",
    "\n",
    "    # Compute geometric properties for each cluster\n",
    "    tls_cluster_data = analyze_clusters(tls_stems_points, tls_clusters)\n",
    "    dls_cluster_data = analyze_clusters(dls_stems_points, dls_clusters)\n",
    "\n",
    "    # Compute weighted scores for clusters based on reliability\n",
    "    tls_cluster_data = calculate_weighted_score(tls_cluster_data)\n",
    "    dls_cluster_data = calculate_weighted_score(dls_cluster_data)\n",
    "\n",
    "    # Sort clusters by score in descending order\n",
    "    sorted_tls_clusters = sorted(tls_cluster_data.items(), key=lambda x: x[1]['score'], reverse=True)\n",
    "    sorted_dls_clusters = sorted(dls_cluster_data.items(), key=lambda x: x[1]['score'], reverse=True)\n",
    "\n",
    "    # Ensure valid clusters exist before proceeding\n",
    "    if not sorted_tls_clusters:\n",
    "        raise ValueError(\"No valid TLS clusters detected!\")\n",
    "    if not sorted_dls_clusters:\n",
    "        raise ValueError(\"No valid DLS clusters detected!\")\n",
    "\n",
    "    # Select the top 20% of clusters based on their scores\n",
    "    top_tls_clusters = [label for label, _ in sorted_tls_clusters[:int(len(sorted_tls_clusters) * 0.2)]]\n",
    "    top_dls_clusters = [label for label, _ in sorted_dls_clusters[:int(len(sorted_dls_clusters) * 0.2)]]\n",
    "\n",
    "    # Filter points belonging to the selected clusters\n",
    "    tls_filtered_points = tls_stems_points[np.isin(tls_clusters, top_tls_clusters)]\n",
    "    dls_filtered_points = dls_stems_points[np.isin(dls_clusters, top_dls_clusters)]\n",
    "\n",
    "    # Construct tree trunk graphs using kNN (k=3)\n",
    "    tls_graph_features, tls_node_to_node_distances = construct_graph_features(tls_filtered_points, k=3)\n",
    "    dls_graph_features, dls_node_to_node_distances = construct_graph_features(dls_filtered_points, k=3)\n",
    "\n",
    "    # Define matching thresholds for distance and angle constraints\n",
    "    distance_threshold = 0.5  # Euclidean distance threshold (meters)\n",
    "    angle_threshold = 10      # Angular difference threshold (degrees)\n",
    "\n",
    "    # Perform graph-based matching using tree trunk structures\n",
    "    matched_sets = match_sets(dls_graph_features, tls_graph_features, dls_node_to_node_distances, tls_node_to_node_distances, distance_threshold, angle_threshold)\n",
    "\n",
    "    # Record all matched tree trunk pairs\n",
    "    recorded_matches = record_matches(matched_sets)\n",
    "\n",
    "    # Identify complete triangle patterns among matched sets\n",
    "    complete_patterns = find_complete_patterns(recorded_matches)\n",
    "\n",
    "    # Solve optimization problem to refine matching results\n",
    "    matched_x_pairs, matched_y_patterns = optimize_graph_matching(complete_patterns)\n",
    "\n",
    "    # Extract final matched indices for TLS and DLS\n",
    "    distance_threshold = 0.5  # Euclidean distance threshold (meters)\n",
    "    angle_threshold = 10      # Angular difference threshold (degrees)\n",
    "\n",
    "\n",
    "    # Retrieve 3D coordinates of matched trees using DTM\n",
    "    dls_3d_points = get_z_elevation_from_dtm(dls_filtered_points, dls_dtm_data, dls_indices)\n",
    "    tls_3d_points = get_z_elevation_from_dtm(tls_filtered_points, tls_dtm_data, tls_indices)\n",
    "\n",
    "    # Compute the rigid body transformation matrix\n",
    "    transformation_matrix = compute_transformation_matrix(dls_3d_points, tls_3d_points)\n",
    "\n",
    "    # Output the final transformation matrix\n",
    "    print(\"Final Transformation Matrix:\")\n",
    "    print(transformation_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca191e55-cef4-4053-8df8-ab3bc788ab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
